{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2MAS_Unet_segmentaiton.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4yz7R8ONr5z2oPiXqHTqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/M2MAS_neural_networks/blob/main/M2MAS_Unet_segmentaiton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLNSJ1i4MIEh"
      },
      "source": [
        "# U-net for crack segmentation\n",
        "### Assignment for M2MAS \"Réseaux de Neurones profonds pour l'Apprentissage\"\n",
        "\n",
        "*References:*\n",
        " * U-Net: Convolutional Networks for Biomedical Image Segmentation, Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention (MICCAI), Springer, LNCS, Vol.9351: 234--241, 2015, available at [arXiv:1505.04597](http://arxiv.org/abs/1505.04597)\n",
        " * An 'All Terrain' Crack Detector Obtained by Deep Learning on Available Databases, Sébastien Drouyer, Image Processing On Line, 10 (2020), pp. 105–123. https://doi.org/10.5201/ipol.2020.282\n",
        "\n",
        "\n",
        "### Modalities:\n",
        "This assignment has to be send by email to bruno.galerne@univ-orleans.fr before Sunday March 7, 23:59.\n",
        "Each question should be answered by providing a new section of the notebook (starting with a text cell ```# Question 1```) with all necessary code.\n",
        "Code must be commented and introduced with a text cell. Results must be discussed in a text cell. In case of difficulty, you can also discuss errors in a text cell.\n",
        "\n",
        "Each training of the neural network should be limited to 20 epochs. Note that the computation time will be arround 2 hours on a K80 GPU, and faster with newer GPUs.\n",
        "\n",
        "### Questions:\n",
        "First read and run the proposed notebook to get a first version of the network.\n",
        "1. **Performance evaluation:** Explain (in a text cell) what is computed by\n",
        "```\n",
        "valscores = test_performance(unet, valset)\n",
        "testscores = test_performance(unet, testset)\n",
        "```\n",
        "\n",
        "1. **First training:** \n",
        "  * Improve the proposed code by adding the capacity to save the trained model to disk and load the model from disk (see https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference)\n",
        "  * Train the model for 20 epochs (only 2 while developing the code...). \n",
        "  * Store the training loss at the end of each epoch. At the end of training plot the epoch loss for each epoch.\n",
        "  * Download the model parameters and/or save them to your google drive for a reference.\n",
        "  * Reload this models as ```unet_ref``` and check the reload is OK.\n",
        "  * Evaluate and report its performance scores using ```test_performance``` on the testset.\n",
        "1. **Training with data augmentation:**\n",
        "  * Explain in a few sentences the concept and interest of data augmentation. \n",
        "  * Discuss a data augmentation strategy for the proposed dataset and implement it within ```class CRACK500(Dataset):```\n",
        "  * Retrain the U-net with data augmentation (starting from a random initialization) for 20 epochs.\n",
        "  * Save and reload the model as  ```unet_data_aug```.\n",
        "  * Compare with ```unet_ref``` regarding training loss and performance on test set.\n",
        "1. **Early stopping:** Using the data augmentation of the previous question:\n",
        "  * At each epoch, evaluate the performance on the validation set and save the network with the best F1 score on validation set.\n",
        "  * Save and reload the retained model as  ```unet_data_aug_early_stop```.\n",
        "  * Compare to other models regarding performance on the test set. What is the best model among the three trainings?\n",
        "\n",
        "**Bonus question (on 3 points/20): Training using BatchNormalization layers:**\n",
        "  * Recall the principle of BatchNormalization layers\n",
        "  * Define a new class Unet_bn network model by adding one batch normalization layer before each max pool and up-convolution.\n",
        "  * Train this new network using data augmentation. Save it.\n",
        "  * Compare the performance of this new network. Make sure to be in evaluation mode for testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jusPdhQF_e0y"
      },
      "source": [
        "# Provided notebook (not to be changed)\n",
        "*This whole section should not be changed.\n",
        "You may copy and change cells into your sections.\n",
        "You can suppose that the cells of the provided notebook have been executed before your answers (no need to copy everything, only the changes)* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7cSMsoRMBG3"
      },
      "source": [
        "## Check that the notebook has an active GPU:\n",
        "(otherwise go to Edit->Notebook properties ->...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_ouWaCSMED9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Fol-ncG7Lg"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7JJmfxaIfZn"
      },
      "source": [
        "Download CRACK500 datatset:\n",
        " * Links are here:\n",
        "https://drive.google.com/drive/folders/1y9SxmmFVh0xdQR-wdchUmnScuWMJ5_O-\n",
        " * Reference work is here:\n",
        "https://github.com/fyangneil/pavement-crack-detection\n",
        "\n",
        "CRACK500\n",
        " * Format :640 x 360(60kb, jpeg)\n",
        " * Dataset Size : Total 3368 Images - Train(1896 Images), Val(348 Images), Test(1124 Images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBwhutKmHeQR"
      },
      "source": [
        "# Original data, not used:\n",
        "# # traindata.zip\n",
        "# !gdown --id 1iOaEWPJDY4U5s0BOOb1fIjcHWDc7FhX-\n",
        "# # testdata.zip\n",
        "# !gdown --id 1C7pj7BP32Qqpm6q8QmirJfjc37rhPdZh\n",
        "# # valdata.zip \n",
        "# !gdown --id 1fUd1BBh2BRWdbTe5p8R5NK-eBzhZYumw\n",
        "\n",
        "# Images:\n",
        "# traincrop.zip\n",
        "!gdown --id 1Qgex6YDpQ0yRrD8JFtNggYN30Ner3rY8\n",
        "# testcrop.zip\n",
        "!gdown --id 1u7wuaQHWWUtF5ON0MhGXcjwbfItniIK5\n",
        "# valcrop.zip \n",
        "!gdown --id 16eqMf5E9C-eQxGl-ATCQjm4zfLkCkxNn\n",
        "# Download .txt files describing datasets:\n",
        "# train.txt\n",
        "!gdown --id 1278gNvcfmGQ3yPw3i9v6ATkLxm0-5Iyk\n",
        "# test.txt\n",
        "!gdown --id 1Sq_jnVulc3cspnSXzEvBxpy6AV8D5FwD\n",
        "# val.txt \n",
        "!gdown --id 1zrLC6qjJNYOOeRGrlBnZHgv9BKZMj2f2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inTt3c-mK9Y7"
      },
      "source": [
        "Unzip data (-q=quietly):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R572oUEJK8Ys"
      },
      "source": [
        "# !unzip traindata.zip\n",
        "# !unzip testdata.zip\n",
        "# !unzip valdata.zip\n",
        "!unzip -q traincrop.zip\n",
        "!unzip -q testcrop.zip\n",
        "!unzip -q valcrop.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G98B_QUfT78Y"
      },
      "source": [
        "#PyTorch Dataloaders:\n",
        "\n",
        "Create a data loader for the train, test, val datasets.\n",
        "We follow mostly:\n",
        "WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znqzypfvS-4V"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "import os\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmT9rqfaaLwZ"
      },
      "source": [
        "# image lists are opened using panda:\n",
        "trainlist = pd.read_csv('train.txt', sep=' ',header=None)\n",
        "print(trainlist)\n",
        "print(trainlist.shape)\n",
        "print(len(trainlist))\n",
        "print(trainlist.iloc[3,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouHSfWHxZY0f"
      },
      "source": [
        "\n",
        "class CRACK500(Dataset):\n",
        "    \"\"\"CRACK500 dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, txt_file, root_dir, train=False, data_augmentation=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            txt_file (string): Path to the txt file list of image paths (eg train.txt, test.txt, val.txt).\n",
        "            root_dir (string): Parent directory for the directory with all the images.\n",
        "            train (boolean): True for training dataset (crop images to have size 256x256, otherwise get full image)\n",
        "            data_augmentation (boolean): True for using data_augmentation during training  \n",
        "        \"\"\"\n",
        "        self.imgslist = pd.read_csv(txt_file, sep=' ',header=None)\n",
        "        self.root_dir = root_dir\n",
        "        self.train = train\n",
        "        self.data_augmentation = data_augmentation\n",
        "    def __len__(self):\n",
        "        return len(self.imgslist)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get image and mask paths\n",
        "        img_name = os.path.join(self.root_dir, self.imgslist.iloc[idx, 0])\n",
        "        mask_name = os.path.join(self.root_dir, self.imgslist.iloc[idx, 1])\n",
        "        \n",
        "        # open images using PIL and apply transform list:\n",
        "        image = Image.open(img_name)\n",
        "        image = transforms.ToTensor()(image)\n",
        "        image = transforms.Grayscale()(image)\n",
        "        image = transforms.Normalize(0.5, 0.5)(image)\n",
        "\n",
        "        # open mask binary image using PIL:\n",
        "        mask = Image.open(mask_name)\n",
        "        mask = transforms.ToTensor()(mask)\n",
        "        mask = mask.long().squeeze()\n",
        "\n",
        "        if self.train: #get a subimage of size 256x256:\n",
        "          if self.data_augmentation:\n",
        "            \n",
        "            # TODO:\n",
        "            print('NOT IMPLEMENTED YET')\n",
        "\n",
        "          else: \n",
        "            # crop center 256x256:\n",
        "            h,w = mask.size()\n",
        "            c1 = (h-256)//2\n",
        "            c2 = (w-256)//2\n",
        "            image = image[:,c1:(c1+256),c2:(c2+256)]\n",
        "            mask = mask[c1:(c1+256),c2:(c2+256)]\n",
        "        sample = [image, mask]          \n",
        "        return sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hebd55FLIxCN"
      },
      "source": [
        "## Test dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AUn-6BKvRqO"
      },
      "source": [
        "trainset = CRACK500(txt_file='train.txt', root_dir='', train=True, data_augmentation=False) \n",
        "image, mask = trainset[100]\n",
        "print(image.size())\n",
        "print(image.dtype)\n",
        "print(mask.size())\n",
        "print(mask.dtype)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
        "ax[0].imshow(image.squeeze(), cmap=plt.cm.gray)\n",
        "ax[0].set_title(\"Image\")\n",
        "ax[1].imshow(mask, cmap=plt.cm.gray)\n",
        "ax[1].set_title(\"True Mask\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "testset = CRACK500(txt_file='test.txt', root_dir='', train=False, data_augmentation=False) \n",
        "image, mask = testset[200]\n",
        "print(image.size())\n",
        "print(image.dtype)\n",
        "print(mask.size())\n",
        "print(mask.dtype)\n",
        "\n",
        "valset = CRACK500(txt_file='val.txt', root_dir='', train=False, data_augmentation=False) \n",
        "image, mask = valset[21]\n",
        "print(image.size())\n",
        "print(image.dtype)\n",
        "print(mask.size())\n",
        "print(mask.dtype)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
        "ax[0].imshow(image.squeeze(), cmap=plt.cm.gray)\n",
        "ax[0].set_title(\"Image\")\n",
        "ax[1].imshow(mask, cmap=plt.cm.gray)\n",
        "ax[1].set_title(\"True Mask\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l7GtXyOgEyd"
      },
      "source": [
        "# check full data_loader: (sanity check, useful only to check that all images are OK)\n",
        "for k, dataset in enumerate([trainset, testset, valset]):\n",
        "  print(['DATASET: ', k])\n",
        "  for i in range(len(dataset)):\n",
        "    image, mask = trainset[i]\n",
        "    # if image.shape[0] != 360:\n",
        "    #   print([i,image.shape])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP4Jo_yOVmBK"
      },
      "source": [
        "## U-net version with same size (as in IPOL paper)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRKH_aMAVixI"
      },
      "source": [
        "#double 3x3 convolution \n",
        "def dual_conv(in_channel, out_channel):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, kernel_size=3,padding=1,padding_mode='reflect'),\n",
        "        nn.ReLU(inplace= True),\n",
        "        nn.Conv2d(out_channel, out_channel, kernel_size=3,padding=1,padding_mode='reflect'),\n",
        "        nn.ReLU(inplace= True),\n",
        "    )\n",
        "    return conv\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        # Left side (contracting path)\n",
        "        self.dwn_conv1 = dual_conv(1, 64)\n",
        "        self.dwn_conv2 = dual_conv(64, 128)\n",
        "        self.dwn_conv3 = dual_conv(128, 256)\n",
        "        self.dwn_conv4 = dual_conv(256, 512)\n",
        "        self.dwn_conv5 = dual_conv(512, 1024)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        #Right side  (expnsion path) \n",
        "        #transpose convolution is used shown as green arrow in architecture image\n",
        "        self.trans1 = nn.ConvTranspose2d(1024,512, kernel_size=2, stride= 2)\n",
        "        self.up_conv1 = dual_conv(1024,512)\n",
        "        self.trans2 = nn.ConvTranspose2d(512,256, kernel_size=2, stride= 2)\n",
        "        self.up_conv2 = dual_conv(512,256)\n",
        "        self.trans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride= 2)\n",
        "        self.up_conv3 = dual_conv(256,128)\n",
        "        self.trans4 = nn.ConvTranspose2d(128,64, kernel_size=2, stride= 2)\n",
        "        self.up_conv4 = dual_conv(128,64)\n",
        "\n",
        "        #output layer\n",
        "        self.out = nn.Conv2d(64, 2, kernel_size=1)\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        #forward pass for Left side\n",
        "        x1 = self.dwn_conv1(image)\n",
        "        x2 = self.maxpool(x1)\n",
        "        x3 = self.dwn_conv2(x2)\n",
        "        x4 = self.maxpool(x3)\n",
        "        x5 = self.dwn_conv3(x4)\n",
        "        x6 = self.maxpool(x5)\n",
        "        x7 = self.dwn_conv4(x6)\n",
        "        x8 = self.maxpool(x7)\n",
        "        x9 = self.dwn_conv5(x8)\n",
        "        \n",
        "\n",
        "        #forward pass for Right side\n",
        "        x = self.trans1(x9)\n",
        "        x = self.up_conv1(torch.cat([x,x7], 1))\n",
        "\n",
        "        x = self.trans2(x)\n",
        "        x = self.up_conv2(torch.cat([x,x5], 1))\n",
        "\n",
        "        x = self.trans3(x)\n",
        "        x = self.up_conv3(torch.cat([x,x3], 1))\n",
        "\n",
        "        x = self.trans4(x)\n",
        "        x = self.up_conv4(torch.cat([x,x1], 1))\n",
        "        \n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "unet = Unet().to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SnW-1sdJTfT"
      },
      "source": [
        "## Test of untrained Unet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB1OThtXt0a5"
      },
      "source": [
        "# Apply Unet and check sizes:\n",
        "image, mask = trainset[3]\n",
        "out=unet(torch.unsqueeze(image.to(device),0))\n",
        "print('output size: ', out.size())\n",
        "print('mask size: ',mask.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH52nnWFZt_y"
      },
      "source": [
        "#Training of Unet:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr4LbYoUZ8Eg"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(unet.parameters())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2lsuAn5xwyi"
      },
      "source": [
        "##Define data loader for batch training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yYA2TESxK9j"
      },
      "source": [
        "dataloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)\n",
        "n_train = len(trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JH_I3icw7bN"
      },
      "source": [
        "iter_info = 100\n",
        "since = time.time()\n",
        "for epoch in range(2):\n",
        "  since_epoch = time.time()\n",
        "  running_loss = 0.0\n",
        "  epoch_loss = 0.0\n",
        "  for i, data in enumerate(dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    images = data[0].to(device)\n",
        "    masks = data[1].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = unet(images)\n",
        "    loss = criterion(outputs, masks)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    epoch_loss += loss.item()\n",
        "    if i % iter_info == (iter_info-1):    # print every iter_info mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %\n",
        "        (epoch + 1, i + 1, running_loss / iter_info))\n",
        "      running_loss = 0.0\n",
        "  time_elapsed_epoch = time.time() - since_epoch    \n",
        "  print('Epoch completed in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed_epoch // 60, time_elapsed_epoch % 60))\n",
        "  print('Epoch loss: %.3f' % (epoch_loss / n_train))\n",
        "\n",
        "print('Finished Training')\n",
        "time_elapsed = time.time() - since\n",
        "print('Training completed in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hJwtqo3a7Vt"
      },
      "source": [
        "#Visualize some prediction (on cropped val image to be improved for performance evaluation):\n",
        "Images size must be multiple of 32.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saUCfOGCbkMq"
      },
      "source": [
        "\n",
        "i = np.random.randint(len(valset))\n",
        "image, mask = valset[i]\n",
        "print(image.size(), mask.size())\n",
        "h,w = mask.size()\n",
        "nh = (h//32)*32\n",
        "nw = (w//32)*32\n",
        "image = image[:,:nh,:nw]\n",
        "mask = mask[:nh,:nw]\n",
        "print(image.size(), mask.size())\n",
        "\n",
        "with torch.no_grad():\n",
        "  out = unet(torch.unsqueeze(image.to(device),0))\n",
        "  predprob = F.softmax(out,dim=1).to('cpu')\n",
        "predprob1 = predprob[0,0,:,:].squeeze()\n",
        "predprob2 = predprob[0,1,:,:].squeeze()\n",
        "predmask = torch.argmax(out.squeeze(), dim=0).to('cpu')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n",
        "ax[0].imshow(image.squeeze(), cmap=plt.cm.gray)\n",
        "ax[0].set_title(\"Image\")\n",
        "ax[1].imshow(predprob1, cmap=plt.cm.gray)\n",
        "ax[1].set_title(\"Proba map 1\")\n",
        "ax[2].imshow(predprob2, cmap=plt.cm.gray)\n",
        "ax[2].set_title(\"Proba map 2\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n",
        "ax[0].imshow(image.squeeze(), cmap=plt.cm.gray)\n",
        "ax[0].set_title(\"Image\")\n",
        "ax[1].imshow(predmask, cmap=plt.cm.gray)\n",
        "ax[1].set_title(\"Predicted Mask\")\n",
        "ax[2].imshow(mask, cmap=plt.cm.gray)\n",
        "ax[2].set_title(\"True Mask\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4b7v1ToNUY5"
      },
      "source": [
        "## Performance evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FilVOtL381t"
      },
      "source": [
        "def test_performance(unet, dataset):\n",
        "  tp_count = 0\n",
        "  tn_count = 0\n",
        "  fp_count = 0\n",
        "  fn_count = 0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataset):\n",
        "      image = data[0]\n",
        "      mask = data[1]\n",
        "      h,w = mask.size()\n",
        "      nh = (h//32)*32\n",
        "      nw = (w//32)*32\n",
        "      image = image[:,:nh,:nw].to(device)\n",
        "      mask = mask[:nh,:nw].to(device)\n",
        "      out = unet(torch.unsqueeze(image,0))\n",
        "      predmask = torch.argmax(out.squeeze(), dim=0)\n",
        "      # count:\n",
        "      tp = torch.sum(predmask*mask)\n",
        "      fp = torch.sum(predmask*(1.-mask))\n",
        "      tn = torch.sum((1-predmask)*(1.-mask))\n",
        "      fn = torch.sum((1-predmask)*mask)\n",
        "      if not (tp+fp+tn+fn == nh*nw):\n",
        "        print('counting inconstancy')\n",
        "      tp_count = tp_count + tp\n",
        "      tn_count = tn_count + tn\n",
        "      fp_count = fp_count + fp\n",
        "      fn_count = fn_count + fn\n",
        "      if i%100==99:\n",
        "        print('test_performance: Image ', i, ' / ', len(dataset))\n",
        "    print(tp_count, tn_count, fp_count, fn_count)\n",
        "    precision = (tp_count/(tp_count+fp_count)).to('cpu').numpy()\n",
        "    recall = (tp_count/(tp_count+fn_count)).to('cpu').numpy()\n",
        "    F1score =  2. * precision * recall / (precision + recall)\n",
        "    return([precision, recall, F1score])\n",
        "\n",
        "\n",
        "valscores = test_performance(unet, valset)\n",
        "print(valscores)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SolUG9jvLz8y"
      },
      "source": [
        "#Question 1\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTGi1luVN-I7"
      },
      "source": [
        "#Question 2\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hUQ9Ip2OC37"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw89xObmN_wp"
      },
      "source": [
        "#Question 3\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7TP9M7ODqf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teat5rxAOQcl"
      },
      "source": [
        "#Question 4\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvkOi7etOOX8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}