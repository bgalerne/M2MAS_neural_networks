{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_M2MAS_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ml5gAj8EaJr8"
      ],
      "authorship_tag": "ABX9TyPKGptmFSTKCB+fYZhE6scx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/M2MAS_neural_networks/blob/main/4_M2MAS_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV1iu3g19SgB"
      },
      "source": [
        "# Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFR78hGKPlx9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3-nug6USbT5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.autograd as autograd\n",
        "import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr8c4V7LU2uN"
      },
      "source": [
        "#GAN MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69JR0dKRqJM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Load MNIST and define some function to view images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2IkxQGJU8JM"
      },
      "source": [
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "datatrain = datasets.MNIST('.', train=True, download=True,\n",
        "                    transform=transform)\n",
        "batch_size_train = 100\n",
        "trainloader = torch.utils.data.DataLoader(datatrain,batch_size=batch_size_train,\n",
        "                                           shuffle=True,num_workers=1,pin_memory=True)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img/0.5 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "print('Images:')\n",
        "imshow(torchvision.utils.make_grid(images, nrow=10))\n",
        "# print labels\n",
        "print('Labels:')\n",
        "for i in range(10):\n",
        "  print(' '.join('%5s' % str(labels[10*i+j].numpy()) for j in range(10)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNytRCdMYhQX"
      },
      "source": [
        "## Generative and discriminative models:\n",
        "\n",
        "**Exercice**  \n",
        "Define the Generator and discriminative models following the above instructions:\n",
        "\n",
        "*Generator*  \n",
        "Define a class `G_net` with the following architecture:\n",
        "\n",
        "Generator (input : a random vector in the latent space of dimension $d$)\n",
        "$d$ is a parameter of the constructor.  \n",
        "The network applies the following list operations: \n",
        "1. Fully connected layer, output size 256\n",
        "2. Leaky ReLU ($\\alpha = 0.2$) activation\n",
        "3. Fully connected layer, output size 512 \n",
        "4. Leaky ReLU ($\\alpha = 0.2$) activation \n",
        "5. Fully connected layer, output size 784 \n",
        "6. Tanh activation\n",
        "7. Reshape (1x28Ã—28) (use `torch.view()`)\n",
        "\n",
        "*Discriminator*  \n",
        "Define a class `D_net` with the following architecture:\n",
        "1. Flatten images to 1D tensors of size 784.\n",
        "2. Fully connected layer, output size 512\n",
        "3. Leaky ReLU ($\\alpha = 0.2$) activation\n",
        "4. Fully connected layer, output size 256\n",
        "5. Leaky ReLU ($\\alpha = 0.2$) activation\n",
        "6. Fully connected layer, output size 1\n",
        "\n",
        "Define instances of the networks using $d=32$.\n",
        "\n",
        "**Remark:** One should add a sigmoid layer to the discriminator to output a probability but this is implicitly done in the criterion (see next exercice).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZDqEuBSY1Of"
      },
      "source": [
        "# Generator network:\n",
        "class G_Net(nn.Module):\n",
        "  #TODO\n",
        "\n",
        "\n",
        "# Discriminator network:\n",
        "class D_Net(nn.Module):\n",
        "  #TODO\n",
        "\n",
        "d = 32\n",
        "G_net = G_Net(d).to(device)\n",
        "D_net = D_Net().to(device)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGSzc-Jgc5CO"
      },
      "source": [
        "## View some generated images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8bGzguyc9h9"
      },
      "source": [
        "def show_G_net():\n",
        "  with torch.no_grad():\n",
        "    z = torch.randn(100,d).to(device)\n",
        "    genimages = G_net(z)\n",
        "    plt.figure(figsize = (5,5))\n",
        "    imshow(torchvision.utils.make_grid(genimages.to('cpu'),nrow=10))\n",
        "    #print(disnet(genimages))\n",
        "    plt.show()\n",
        "\n",
        "show_G_net()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPoe6I3-eHai"
      },
      "source": [
        "## GAN training code\n",
        "\n",
        "In PyTorch, \n",
        "the function `nn.BCEWithLogitsLoss` combines a `Sigmoid` layer and the `BCELoss`, that is,  for $(x,y)\\in\\mathbb{R}\\times \\{0,1\\}$, \n",
        "$$\n",
        "\\ell(x,y) =  -y \\cdot \\log \\sigma(x)\n",
        "        - (1 - y) \\cdot \\log (1 - \\sigma(x))\n",
        "$$\n",
        "where $\\sigma: \\mathbb{R}\\to (0,1)$ is the sigmoid function defined by\n",
        "$$\n",
        "\\sigma(x) = \\frac{e^x}{1+e^{x}} = \\frac{1}{1+e^{-x}}.\n",
        "$$\n",
        "The sigmoid function plays the role of the `softmax` function for binary classification since it maps $\\mathbb{R}\\to (0,1)$ to produce the probability of being in the class $y=1$ (and then $1 - \\sigma(x)$ is the probability of being in the class $y=0$).\n",
        "\n",
        "In the course formula of the discriminator loss, \n",
        "$$\n",
        "      \\max_{\\theta_d}\n",
        "      \\underbrace{\n",
        "        \\frac 1 n\n",
        "        \\sum_{x_{\\text{real}} \\in \\mathcal{T}_{\\text{real}}} \\log D_{\\theta_d}(x_{\\text{real}})}_{\n",
        "        \\substack{\n",
        "          \\text{force predicted labels to be 1}\\\\\n",
        "          \\text{for real images}\n",
        "        }\n",
        "      }\n",
        "      +\n",
        "      \\underbrace{\n",
        "        \\frac1m\n",
        "        \\sum_{x_{\\text{fake}} \\in \\mathcal{T}_{\\text{fake}}} \\log (1 - D_{\\theta_d}(x_{\\text{fake}}))}_{\n",
        "        \\substack{\n",
        "          \\text{force predicted labels to be 0}\\\\\n",
        "          \\text{for fake images}\n",
        "        }}\n",
        "$$\n",
        "the sigmoid layer is implicitly included in $D_{\\theta_d}$, but this will not be the case in the PyTorch implementation.\n",
        "In short, \n",
        "$$\n",
        "D_{\\theta_d}(x) = \\sigma(\\mathtt{D\\_{net}}(x)).\n",
        "$$\n",
        "\n",
        "**Exercice**\n",
        "\n",
        "Implement the following training algorithm, where $b$ is the batch size (and we take $m=n=b$ in the loss formula):\n",
        "\n",
        "\n",
        "> For each batch of images $x_{\\text{real}}$:\n",
        "> > **1) Train discriminator:**\n",
        "> > > Generate $z$ a tensor of size $b\\times d$ of idd Gaussian variables  \n",
        "> > > Generate  $x_{\\text{fake}} = \\mathtt{G\\_{net}}(z)$ a set $b$ fake images  \n",
        "> > > Compute the (opposite of the) loss to minimize for the discriminator using `nn.BCEWithLogitsLoss`\n",
        "> > > Compute the gradient and do an optimize step for the disciminator parameters  \n",
        "\n",
        "> > **2) Train the generator:**\n",
        "> > > Generate $z$ a new tensor of size $b\\times d$ of idd Gaussian variables  \n",
        "> > > Compute the loss to minimize\n",
        "$$\n",
        "      \\underbrace{\n",
        "        - \\frac 1 m\n",
        "        \\sum_{z \\in \\mathcal T_{\\text{rand}}} \\log D_{\\theta_d}(G_{\\theta_g}(z)))\n",
        "      }_{\n",
        "        \\substack{\n",
        "          \\text{force the discriminator to think that}\\\\\n",
        "          \\text{our generated fake images are real (close to 1)}\n",
        "        }\n",
        "      }\n",
        "$$\n",
        "using `nn.BCEWithLogitsLoss`  \n",
        "Compute the gradient and do an optimize step for the disciminator parameters\n",
        "\n",
        "Train the networks for 20 epochs using batch size $b=100$.\n",
        "Display the current losses and show generated images at the end of each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J-h9XnZeGrz"
      },
      "source": [
        "\n",
        "D_lr = 0.0002\n",
        "G_lr = 0.0002\n",
        "G_optimizer = optim.Adam(G_net.parameters(), lr=D_lr,betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D_net.parameters(), lr=G_lr,betas=(0.5, 0.999))\n",
        "\n",
        "# mode collapse with SGD:\n",
        "#G_optimizer = optim.SGD(G_net.parameters(), lr=lr, momentum=0.9)\n",
        "#D_optimizer = optim.SGD(D_net.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6CIY-YHfMq5"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXbp5DXM__0K"
      },
      "source": [
        "## Interpolation in latent space:\n",
        "\n",
        "**Exercice:** \n",
        "\n",
        "Generate 2 sets of 10 latent variable $z_0$ and $z_1$ and display the generated images by the latent variables:\n",
        "$$\n",
        "z_\\theta = (1-\\theta) z_0 + \\theta z_1\n",
        "$$\n",
        "for $\\theta$ varying between $0$ and $1$ (using the `torch.linspace` function with 20 intermediate values).\n",
        "Display all the images in a grid of height 10 and width 20 images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lATPnHSTAFIP"
      },
      "source": [
        "with torch.no_grad():\n",
        "  nlatent = 10\n",
        "  ninterp = 20\n",
        "  z0 = torch.randn(nlatent,d).to('cuda')\n",
        "  z1 = torch.randn(nlatent,d).to('cuda')\n",
        "\n",
        "  #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpQbbn9l50OQ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzoqzzE5oku"
      },
      "source": [
        "#Sources:\n",
        "Load MNIST: https://github.com/pytorch/examples/blob/master/mnist/main.py  \n",
        "GAN architecture: TP GAN by Alasdair Newson: https://sites.google.com/site/alasdairnewson/teaching.  \n",
        "Another more complex model: https://github.com/RAKIYOU/Training-GAN-on-MNIST"
      ]
    }
  ]
}